import pandas as pd
import json
import time
import os
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
FILE_PATH = "/Users/seungwan/Downloads/ìœ ì¹˜ì›ë°ì´í„°250310.xlsx"  # íŒŒì¼ëª… ìˆ˜ì • ê°€ëŠ¥
CACHE_FILE = "/Users/seungwan/Downloads/address_cache.json"
BATCH_SIZE = 100  # ì¹´ì¹´ì˜¤ API ì œí•œì„ ê³ ë ¤í•˜ì—¬ ì‘ê²Œ ì„¤ì •
MAX_WORKERS = 3  # ë™ì‹œ ìš”ì²­ ìˆ˜ë¥¼ ì œí•œ

def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            print("âŒ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ìƒˆ ìºì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤")
    return {}

def save_cache():
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(address_cache, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ì£¼ì†Œ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(address_cache)}ê°œ ì£¼ì†Œ")

def preprocess_row(row):
    return {
        "name": str(row.get('ìœ ì¹˜ì›ëª…', '')).strip(),
        "address": str(row.get('ì£¼ì†Œ', '')).strip(),
        "latitude": None,
        "longitude": None
    }

def geocode_address_kakao(item):
    address = item["address"]
    name = item["name"]

    if not address:
        return item

    # ìºì‹œ í™•ì¸
    if address in address_cache:
        item["latitude"], item["longitude"] = address_cache[address]
        return item

    # ì¹´ì¹´ì˜¤ API URLê³¼ í—¤ë” ì„¤ì •
    url = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": "KakaoAK ì¹´ì¹´ì˜¤ API í‚¤"}

    # ì£¼ì†Œë¡œ ë¨¼ì € ê²€ìƒ‰
    for attempt in range(3):  # ìµœëŒ€ 3ë²ˆ ì¬ì‹œë„
        try:
            params = {"query": address}
            response = requests.get(url, headers=headers, params=params)

            if response.status_code == 200:
                result = response.json()
                if result["documents"]:
                    # ì„±ê³µì ìœ¼ë¡œ ì¢Œí‘œë¥¼ ì°¾ìŒ
                    x = float(result["documents"][0]["x"])  # ê²½ë„
                    y = float(result["documents"][0]["y"])  # ìœ„ë„
                    item["longitude"] = x
                    item["latitude"] = y
                    address_cache[address] = (y, x)
                    return item
                else:
                    # ì£¼ì†Œë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° ìœ ì¹˜ì›ëª…ê³¼ ì£¼ì†Œ ì¡°í•©ìœ¼ë¡œ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œë„
                    keyword_url = "https://dapi.kakao.com/v2/local/search/keyword.json"
                    keyword_params = {"query": f"{name} {address.split()[0]}"}  # ìœ ì¹˜ì›ëª…ê³¼ ì£¼ì†Œ ì•ë¶€ë¶„ ì‚¬ìš©

                    keyword_response = requests.get(keyword_url, headers=headers, params=keyword_params)
                    if keyword_response.status_code == 200:
                        keyword_result = keyword_response.json()
                        if keyword_result["documents"]:
                            x = float(keyword_result["documents"][0]["x"])
                            y = float(keyword_result["documents"][0]["y"])
                            item["longitude"] = x
                            item["latitude"] = y
                            address_cache[address] = (y, x)
                            return item

            # API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆê±°ë‚˜ ë‹¤ë¥¸ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
            if response.status_code != 200:
                print(f"âš ï¸ API ì˜¤ë¥˜ (ì½”ë“œ {response.status_code}): {address}")
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„

        except Exception as e:
            print(f"â³ ì¬ì‹œë„ {attempt + 1}/3: {address} ({e})")
            time.sleep(2 ** attempt)

    return item

def chunk_data(data_list, chunk_size):
    for i in range(0, len(data_list), chunk_size):
        yield data_list[i:i + chunk_size]

def process_data():
    print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    df = pd.read_excel(FILE_PATH, engine="openpyxl", header=2)
    df.rename(columns=lambda x: str(x).strip(), inplace=True)
    print("ğŸ“Œ ì»¬ëŸ¼ëª… í™•ì¸:", df.columns)

    kindergartens = [preprocess_row(row) for _, row in df.iterrows()]
    print(f"ì´ {len(kindergartens)}ê°œì˜ ìœ ì¹˜ì› ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")

    batches = list(chunk_data(kindergartens, BATCH_SIZE))
    all_results = []

    for batch_idx, batch in enumerate(batches, 1):
        print(f"\nâ³ ë°°ì¹˜ {batch_idx}/{len(batches)} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ í•­ëª©)")
        batch_results = []

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_item = {executor.submit(geocode_address_kakao, item): item for item in batch}

            for future in tqdm(as_completed(future_to_item), total=len(batch), desc=f"ë°°ì¹˜ {batch_idx} ì§€ì˜¤ì½”ë”©"):
                try:
                    result = future.result()
                    batch_results.append(result)
                except Exception as exc:
                    print(f'ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}')

                # API ì†ë„ ì œí•œì„ í”¼í•˜ê¸° ìœ„í•œ ëŒ€ê¸°
                time.sleep(0.2)  # ì¹´ì¹´ì˜¤ APIëŠ” ì´ˆë‹¹ 10íšŒ ì´í•˜ ìš”ì²­ ê¶Œì¥

                if len(batch_results) % 50 == 0:
                    save_cache()

        all_results.extend(batch_results)
        save_cache()

        # ê° ë°°ì¹˜ ì²˜ë¦¬ í›„ ì¢€ ë” ê¸´ ëŒ€ê¸° ì‹œê°„
        if batch_idx < len(batches):
            print(f"ğŸ•’ ë‹¤ìŒ ë°°ì¹˜ ì „ ëŒ€ê¸° ì¤‘... (5ì´ˆ)")
            time.sleep(5)

    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    result_file = 'kindergartens_only_coords_kakao.json'
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    # ì„±ê³µë¥  ê³„ì‚° ë° ë³´ê³ 
    failed_count = sum(1 for k in all_results if k['address'] and k['latitude'] is None)
    success_rate = ((len(all_results) - failed_count) / len(all_results)) * 100

    print(f"\nâœ… ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ìœ ì¹˜ì› ì¤‘ {len(all_results) - failed_count}ê°œ ì£¼ì†Œ ë³€í™˜ ì„±ê³µ ({success_rate:.2f}%)")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {result_file}")

    # ì‹¤íŒ¨í•œ í•­ëª© ì €ì¥
    if failed_count > 0:
        failed_items = [k for k in all_results if k['address'] and k['latitude'] is None]
        failed_file = 'failed_geocoding_items.json'
        with open(failed_file, 'w', encoding='utf-8') as f:
            json.dump(failed_items, f, ensure_ascii=False, indent=2)
        print(f"âš ï¸ ë³€í™˜ ì‹¤íŒ¨í•œ {failed_count}ê°œ í•­ëª©: {failed_file}")

if __name__ == "__main__":
    address_cache = load_cache()
    process_data()